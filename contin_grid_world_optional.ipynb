{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ContinuousWindyGridworld:\n",
    "    def __init__(self):\n",
    "        # Environment bounds (grid size)\n",
    "        self.x_min, self.x_max = 0, 10  # X-axis limits\n",
    "        self.y_min, self.y_max = 0, 8   # Y-axis limits\n",
    "        self.start = np.array([0.5, 3.5])  # Starting position of the agent\n",
    "        self.goal = np.array([7.5, 3.5])  # Goal position of the agent\n",
    "        \n",
    "        # Wind strength along the x-axis (affects the agent's movement)\n",
    "        self.wind_strength = [0, 0, 0, 0, 1, 1, 2, 2, 1, 0]\n",
    "        \n",
    "        # Action space: defines continuous actions (radius and angle)\n",
    "        # Radius determines how far the agent moves, and angle defines the direction\n",
    "        self.action_space = {\n",
    "            'radius': (0.0, 5.0),  # Maximum step size the agent can take\n",
    "            'angle': (0.0, 2 * np.pi)  # Angle of movement (full circle)\n",
    "        }\n",
    "        \n",
    "        # Initial state of the agent\n",
    "        self.state = self.start.copy()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the start state.\"\"\"\n",
    "        self.state = self.start.copy()  # Reset agent's position\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes an action and returns (next_state, reward, done).\n",
    "        \n",
    "        action: a tuple containing (radius, angle)\n",
    "        radius: distance to move\n",
    "        angle: direction to move in radians\n",
    "        \"\"\"\n",
    "        radius, angle = action  # Unpack action into radius and angle\n",
    "        \n",
    "        # Calculate the next state based on the current state and action\n",
    "        next_state = self.state + np.array([\n",
    "            radius * np.cos(angle),  # Move in the x-direction\n",
    "            radius * np.sin(angle)   # Move in the y-direction\n",
    "        ])\n",
    "        \n",
    "        # Add wind effect based on the x-position of the agent\n",
    "        # Wind strength varies based on the agent's x-coordinate\n",
    "        column = int(np.clip(next_state[0], self.x_min, self.x_max - 1))  # Ensure x is within bounds\n",
    "        wind_effect = self.wind_strength[column]  # Get wind strength at the current x-position\n",
    "        next_state[1] += wind_effect  # Apply wind effect to the y-position\n",
    "        \n",
    "        # Clip the next state to ensure the agent stays within the grid boundaries\n",
    "        next_state[0] = np.clip(next_state[0], self.x_min, self.x_max)  # Clip x to grid bounds\n",
    "        next_state[1] = np.clip(next_state[1], self.y_min, self.y_max)  # Clip y to grid bounds\n",
    "        \n",
    "        self.state = next_state  # Update the agent's state\n",
    "        \n",
    "        # Calculate the reward based on the distance to the goal\n",
    "        distance_to_goal = np.linalg.norm(self.state - self.goal)  # Euclidean distance to the goal\n",
    "        if distance_to_goal < 0.5:\n",
    "            reward = 100  # Reward for reaching the goal\n",
    "            done = True  # The episode ends when the goal is reached\n",
    "        else:\n",
    "            reward = -0.1 * distance_to_goal  # Penalize the agent for being far from the goal\n",
    "            done = False  # The episode continues if the goal is not reached\n",
    "        \n",
    "        return self.state, reward, done  # Return the next state, reward, and done status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "\n",
    "class SGDPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=2, hidden_dim=16):\n",
    "        \"\"\"\n",
    "        Initializes the policy network with a fully connected neural network.\n",
    "\n",
    "        state_dim: The dimensionality of the state input (default is 2 for (x, y) positions).\n",
    "        hidden_dim: The number of units in the hidden layer (default is 16).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the architecture of the neural network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),  # Input layer (state_dim -> hidden_dim)\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Hidden layer\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(hidden_dim, 2)  # Output layer: 2 outputs (mean of radius and angle)\n",
    "        )\n",
    "        \n",
    "        # Adam optimizer for training the model's parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "        # Lists to store actions and rewards during the episode for later update\n",
    "        self.saved_actions = []  \n",
    "        self.rewards = []  \n",
    "        \n",
    "        # Standard deviation parameters for radius and angle (learnable)\n",
    "        self.radius_std = nn.Parameter(torch.tensor(1.0))  \n",
    "        self.angle_std = nn.Parameter(torch.tensor(1.0))  \n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network to get the mean of the action distributions.\n",
    "        \n",
    "        state: The current state input to the network (e.g., position of the agent).\n",
    "        \n",
    "        Returns:\n",
    "        radius_dist: A Normal distribution for the radius (distance to move).\n",
    "        angle_dist: A Normal distribution for the angle (direction of movement).\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state)  # Convert state to a tensor\n",
    "        \n",
    "        # Pass state through the neural network to get raw output\n",
    "        output = self.network(state)\n",
    "        \n",
    "        # Apply sigmoid to scale outputs to valid action ranges (radius and angle)\n",
    "        radius_mean = torch.sigmoid(output[0]) * 5.0 + 1e-6  # Radius is scaled to (0, 5)\n",
    "        angle_mean = torch.sigmoid(output[1]) * 2 * torch.pi + 1e-6  # Angle is scaled to (0, 2*pi)\n",
    "        \n",
    "        # Define the action distributions with learnable standard deviations\n",
    "        radius_dist = D.Normal(radius_mean, torch.clamp(torch.exp(self.radius_std), min=1e-6, max=2.0))\n",
    "        angle_dist = D.Normal(angle_mean, torch.clamp(torch.exp(self.angle_std), min=1e-6, max=2.0))\n",
    "        \n",
    "        return radius_dist, angle_dist\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state using the policy network.\n",
    "        \n",
    "        state: The current state input to the policy network.\n",
    "        \n",
    "        Returns:\n",
    "        radius: The selected radius (distance to move).\n",
    "        angle: The selected angle (direction of movement).\n",
    "        \"\"\"\n",
    "        radius_dist, angle_dist = self.forward(state)  # Get the action distributions\n",
    "        \n",
    "        # Sample from the action distributions to select radius and angle\n",
    "        radius = radius_dist.sample()\n",
    "        angle = angle_dist.sample()\n",
    "        \n",
    "        # Store the log-probabilities of the actions for later use during the update\n",
    "        self.saved_actions.append((radius_dist.log_prob(radius), angle_dist.log_prob(angle)))\n",
    "        \n",
    "        return radius.item(), angle.item()  # Convert tensor values to Python numbers\n",
    "\n",
    "    def update(self, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Updates the policy using the collected rewards and actions.\n",
    "        \n",
    "        gamma: Discount factor for future rewards (default is 0.99).\n",
    "        \"\"\"\n",
    "        R = 0  # Initialize the return (future discounted reward)\n",
    "        returns = []  # List to store the returns for each action\n",
    "        \n",
    "        # Calculate the returns (discounted rewards) in reverse order (from last to first)\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + gamma * R  # Discount the reward\n",
    "            returns.insert(0, R)  # Insert the return at the beginning of the list\n",
    "        \n",
    "        returns = torch.tensor(returns)  # Convert returns list to tensor\n",
    "        \n",
    "        # Normalize the returns (zero mean and unit variance) to improve training stability\n",
    "        if returns.std() > 1e-8:  # Avoid division by zero or very small numbers\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        else:\n",
    "            returns = returns - returns.mean()  # Center the returns, skip scaling\n",
    "\n",
    "        # Compute the policy loss (Negative log-likelihood times the return)\n",
    "        policy_loss = []\n",
    "        for (radius_log_prob, angle_log_prob), R in zip(self.saved_actions, returns):\n",
    "            # Add up the loss for both radius and angle\n",
    "            policy_loss.append(-(radius_log_prob + angle_log_prob) * R)\n",
    "        \n",
    "        self.optimizer.zero_grad()  # Reset gradients before backpropagation\n",
    "        policy_loss = torch.stack(policy_loss).sum()  # Sum up all policy losses\n",
    "        policy_loss.backward()  # Backpropagate the loss\n",
    "        self.optimizer.step()  # Update the model parameters\n",
    "\n",
    "        # Clear stored actions and rewards after the update\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Episode 100/1000, Steps: 408\n",
      "Trial 1, Episode 200/1000, Steps: 868\n",
      "Trial 1, Episode 300/1000, Steps: 184\n",
      "Trial 1, Episode 400/1000, Steps: 58\n",
      "Trial 1, Episode 500/1000, Steps: 14\n",
      "Trial 1, Episode 600/1000, Steps: 92\n",
      "Trial 1, Episode 700/1000, Steps: 77\n",
      "Trial 1, Episode 800/1000, Steps: 287\n",
      "Trial 1, Episode 900/1000, Steps: 929\n",
      "Trial 1, Episode 1000/1000, Steps: 593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m policy\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 26\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     28\u001b[0m     policy\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\konst\\OneDrive\\Desktop\\Code Projects\\ReinforcementLearningUPV\\policies\\sgd_policy.py:32\u001b[0m, in \u001b[0;36mSGDPolicy.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 32\u001b[0m     radius_dist, angle_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     radius \u001b[38;5;241m=\u001b[39m radius_dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     34\u001b[0m     angle \u001b[38;5;241m=\u001b[39m angle_dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\konst\\OneDrive\\Desktop\\Code Projects\\ReinforcementLearningUPV\\policies\\sgd_policy.py:24\u001b[0m, in \u001b[0;36mSGDPolicy.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     23\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m---> 24\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     radius_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(output[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m  \u001b[38;5;66;03m# Scale to action space\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     angle_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(output[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\konst\\miniconda3\\envs\\TradingBot\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1000  # Number of episodes to train the policy for each trial\n",
    "num_trials = 50  # Number of trials to run\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "\n",
    "# Initialize storage for average steps across all trials\n",
    "all_steps = np.zeros((num_trials, num_episodes))  # Array to store steps per episode for each trial\n",
    "\n",
    "# Perform multiple trials\n",
    "for trial in range(num_trials):\n",
    "    env = ContinuousWindyGridworld()  # Initialize the environment (Continuous Windy Gridworld)\n",
    "    policy = SGDPolicy()  # Initialize the policy (SGD-based policy)\n",
    "    \n",
    "    # Iterate over episodes for the current trial\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset the environment to the start state\n",
    "        done = False  # Flag to indicate if the episode is finished\n",
    "        episode_steps = 0  # Counter for steps in this episode\n",
    "        policy.rewards = []  # Reset rewards list for the episode\n",
    "        \n",
    "        # Run the episode until done\n",
    "        while not done:\n",
    "            action = policy.select_action(state)  # Get an action from the policy\n",
    "            next_state, reward, done = env.step(action)  # Take the action in the environment\n",
    "            policy.rewards.append(reward)  # Store the reward for the current step\n",
    "            state = next_state  # Update state\n",
    "            episode_steps += 1  # Increment step counter\n",
    "            \n",
    "            if done:  # If the episode ends, update the policy\n",
    "                policy.update(gamma)\n",
    "        \n",
    "        all_steps[trial, episode] = episode_steps  # Store the number of steps taken for this episode\n",
    "\n",
    "        # Print progress for the first trial every 100 episodes\n",
    "        if (trial == 0) and ((episode + 1) % 100 == 0):  \n",
    "            print(f\"Trial {trial + 1}, Episode {episode + 1}/{num_episodes}, Steps: {episode_steps}\")\n",
    "\n",
    "# Compute average steps over all trials\n",
    "avg_steps_per_episode = all_steps.mean(axis=0)  # Calculate mean steps per episode across trials\n",
    "\n",
    "# Plot the results showing the average steps over episodes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_steps_per_episode, label=\"Average Steps (50 Runs)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Steps to Goal\")\n",
    "plt.title(\"Policy Improvement Over Multiple Trials\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the final policy path (trajectory to the goal)\n",
    "state = env.reset()  # Reset the environment\n",
    "path = [state]  # Initialize list to store path\n",
    "done = False\n",
    "\n",
    "# Follow the final policy path to the goal\n",
    "while not done:\n",
    "    action = policy.select_action(state)  # Select action using the trained policy\n",
    "    next_state, _, done = env.step(action)  # Take action in the environment\n",
    "    path.append(next_state)  # Store the state in the path\n",
    "    state = next_state  # Update state\n",
    "\n",
    "path = np.array(path)  # Convert path list to a numpy array\n",
    "# Plot the trajectory of the final policy\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(path[:, 0], path[:, 1], marker='o')  # Plot the path of the agent\n",
    "plt.scatter(env.goal[0], env.goal[1], c='red', label='Goal')  # Plot the goal position\n",
    "plt.xlim(env.x_min, env.x_max)  # Set X-axis limits\n",
    "plt.ylim(env.y_min, env.y_max)  # Set Y-axis limits\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Final Policy Path\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TradingBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
